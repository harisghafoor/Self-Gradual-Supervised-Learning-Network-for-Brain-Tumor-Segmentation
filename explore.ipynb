{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "\n",
    "from scripts.utils import (_get_device,\n",
    "                        prepare_dataset,\n",
    "                        _get_dataloaders,\n",
    "                        get_normalized_mean,\n",
    "                        get_labelled_indices,\n",
    "                        sabotage_samples)\n",
    "from scripts.trainer import Trainer\n",
    "from scripts.test import evaluate_test_data\n",
    "from scripts.model import Unet\n",
    "from config import Config\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch  # type: ignore\n",
    "import torch.nn as nn  # type: ignore\n",
    "import torch.nn.functional as F  # type: ignore\n",
    "\n",
    "from torch.autograd import Variable  # type: ignore\n",
    "from timeit import default_timer as timer\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "\n",
    "from scripts.utils import prepare_dataset, calculate_metrics,weight_schedule\n",
    "from scripts.test import evaluate_test_data\n",
    "from scripts.loss import (compute_loss, DiceLoss,temporal_loss,consistency_loss,\n",
    "                          pi_model_loss,\n",
    "                          get_labelled_examples_in_batch)\n",
    "from scripts.model import Unet\n",
    "from config import Config\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS\n",
      "Number of labelled samples : 287\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "rng = np.random.RandomState(26)\n",
    "seeds = rng.randint(10000, size=config.n_exps)\n",
    "mode = config.mode\n",
    "RESULTS_DIR = config.RESULTS_DIR\n",
    "THRESHOLD = config.THRESHOLD\n",
    "device = _get_device()\n",
    "os.makedirs(os.path.join(RESULTS_DIR, config.experiment_name), exist_ok=True)\n",
    "# Initialize the seeds\n",
    "seed = seeds[0]\n",
    "# Initialized the object for end to end pipeline\n",
    "model = Unet(img_ch=3, output_ch=1,batch_size = config.BATCH_SIZE,device = device).to(device)\n",
    "ssl = Trainer(seed=seed, device=device, model=model, config_file=config)\n",
    "# Prepare dataset\n",
    "mean_per_channel, std_per_channel = get_normalized_mean(\n",
    "    sorted(glob(os.path.join((config.train_x), \"*\")))\n",
    ")\n",
    "train_dataset, test_dataset = prepare_dataset(\n",
    "    train_x=sorted(glob(os.path.join((config.train_x), \"*\")))[:],\n",
    "    train_y=sorted(glob(os.path.join((config.train_y), \"*\")))[:],\n",
    "    valid_x=sorted(glob(os.path.join((config.valid_x), \"*\")))[:],\n",
    "    valid_y=sorted(glob(os.path.join((config.valid_y), \"*\")))[:],\n",
    "    H=config.H,\n",
    "    W=config.W,\n",
    "    mean=mean_per_channel,\n",
    "    std=std_per_channel,\n",
    ")\n",
    "labelled_indices =  get_labelled_indices(train_dataset.images,RATIO_LABELLED_SAMPLES=config.RATIO_LABELLED_SAMPLES)\n",
    "unlabelled_idxs,train_dataset = sabotage_samples(labelled_indices,train_dataset)\n",
    "assert np.all(np.array(train_dataset.masks)[unlabelled_idxs[0]] == -1)\n",
    "assert np.all(np.array(train_dataset.masks)[labelled_indices[0]] != -1)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=config.SHUFFLE_TRAIN,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=config.SHUFFLE_TEST,\n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# def visualize_images_and_masks(x,y,num_images = 2):\n",
    "#     fig, axs = plt.subplots(num_images, 2, figsize=(5, 5))\n",
    "#     for i in range(num_images):\n",
    "#         axs[i, 0].imshow(np.transpose(x[i].cpu().numpy(), (1, 2, 0)),cmap='gray')\n",
    "#         axs[i, 0].set_title(\"Image {}\".format(i + 1))\n",
    "#         axs[i, 1].imshow(y[i].squeeze().cpu().numpy())\n",
    "#         axs[i, 1].set_title(\"Mask {}\".format(i + 1))\n",
    "#         axs[i,0].set_xticks([])\n",
    "#         axs[i,0].set_yticks([])\n",
    "#         axs[i,1].set_xticks([])\n",
    "#         axs[i,1].set_yticks([])\n",
    "#     # for i in range(3):\n",
    "#     plt.tight_layout()\n",
    "#     plt.xticks([])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_indices = list(unlabelled_idxs)[0:3]\n",
    "# input_image = np.array(train_dataset.images)[sample_indices]\n",
    "# # input_image = np.swapaxes(input_image,axis1 =  1,axis2 = 3)\n",
    "# label = np.array(train_dataset.masks)[sample_indices]\n",
    "# # torch.from_numpy(input_image).shape,torch.from_numpy(label).shape\n",
    "# input_image = torch.from_numpy(input_image).to(device)\n",
    "# label = torch.from_numpy(label).to(device)\n",
    "# input_image = input_image.permute(0, 3,1,2)\n",
    "# label = label.unsqueeze(1)\n",
    "# arr1 = np.zeros((3,1,256,256)).astype(np.float32)\n",
    "# arr2 = np.zeros((3,3,256,256)).astype(np.float32)\n",
    "# tensor1 = torch.from_numpy(arr1).to(ssl.device)\n",
    "# tensor2 = torch.from_numpy(arr2).to(ssl.device)\n",
    "# input_image = torch.concatenate(tensors=(input_image,tensor2))\n",
    "# label  = torch.concatenate(tensors=(tensor1,label))\n",
    "# # label.shape\n",
    "# # input_image.shape,label.shape\n",
    "# model.eval()\n",
    "# predicted_masks = model(input_image)\n",
    "# total_loss, supervised_loss, unsup_loss,nbsup = pi_model_loss(\n",
    "#     actual_mask=label,\n",
    "#     pred_mask=predicted_masks,\n",
    "#     ensemble_mask = label,\n",
    "#     weight= torch.tensor([0.4],requires_grad=True).to(device),\n",
    "#     device=device\n",
    "# )\n",
    "# print(f\" Total loss: {total_loss.item()} \\n Supervised loss: {supervised_loss.item()} \\n Unsupervised loss: {unsup_loss.item()} \\n Number of supervised examples: {nbsup}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Temporal Loss\n",
    "1. Supervised Loss : if label is not -1 then compute cross entropy and dice loss\n",
    "2. Unsupervised Loss : Calcualte the mean squared error loss between the two images i.e output from model and previous epochs averaged output (32,1,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ssl.writer\n",
    "# setup param optimization\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=ssl.config.LR,\n",
    "    betas=(0.9, 0.99)\n",
    ")\n",
    "ntrain = len(train_dataset)\n",
    "n_samples = ssl.config.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     l\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# All batches have been trained\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Update the mean ensemble of outputs\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m Z \u001b[38;5;241m=\u001b[39m (\u001b[43mssl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m \u001b[38;5;241m*\u001b[39m Z) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ssl\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m     74\u001b[0m ) \u001b[38;5;241m*\u001b[39m outputs  \u001b[38;5;66;03m# outputs + Smoothed version of Outputs\u001b[39;00m\n\u001b[1;32m     75\u001b[0m z \u001b[38;5;241m=\u001b[39m Z \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m ssl\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# print loss\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# losses.append(np.mean(batch_losses))\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# saving model\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'alpha'"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.train()\n",
    "losses = []\n",
    "sup_losses = []\n",
    "unsup_losses = []\n",
    "best_loss = 20.0\n",
    "Z = (\n",
    "    torch.zeros(ntrain, 1, ssl.config.H, ssl.config.W).float().to(ssl.device)\n",
    ")  # intermediate values\n",
    "z = (\n",
    "    torch.zeros(ntrain, 1, ssl.config.H, ssl.config.W).float().to(ssl.device)\n",
    ")  # intermediate values\n",
    "outputs = (\n",
    "    torch.zeros(ntrain, 1, ssl.config.H, ssl.config.W).float().to(ssl.device)\n",
    ")  # intermediate values\n",
    "\n",
    "for epoch in range(1):\n",
    "    t = timer()\n",
    "    # evaluate unsupervised cost weight\n",
    "    w = weight_schedule(\n",
    "        epoch,\n",
    "        ssl.config.max_epochs,\n",
    "        ssl.config.max_val,\n",
    "        ssl.config.ramp_up_mult,\n",
    "        ssl.config.k,\n",
    "        ssl.config.n_samples,\n",
    "    )\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\"unsupervised loss weight : {}\".format(w))\n",
    "    # iterate over all batches\n",
    "    # turn it into a usable pytorch object\n",
    "    w = torch.autograd.Variable(torch.FloatTensor([w]), requires_grad=False).to(\n",
    "        ssl.device\n",
    "    )\n",
    "    l = []\n",
    "    supl = []\n",
    "    unsupl = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images).to(ssl.device)\n",
    "        labels = Variable(labels, requires_grad=False).to(ssl.device)\n",
    "        # get output and calculate loss\n",
    "        optimizer.zero_grad()\n",
    "        out = model(images)\n",
    "        # Store the outputs in the tensor\n",
    "        # (batch_size,1,H,W)\n",
    "        z_comp = Variable(\n",
    "            z[i * ssl.config.BATCH_SIZE : (i + 1) * ssl.config.BATCH_SIZE],\n",
    "            requires_grad=False,\n",
    "        ).to(ssl.device)\n",
    "        # compute temporal loss\n",
    "        loss, sup_loss, unsup_loss, nbsup = pi_model_loss(\n",
    "            actual_mask=labels,\n",
    "            pred_mask=out,\n",
    "            ensemble_mask=z_comp,\n",
    "            weight=w,\n",
    "            device=ssl.device,\n",
    "        )\n",
    "        # save the output and losses\n",
    "        outputs[i * ssl.config.BATCH_SIZE : (i + 1) * ssl.config.BATCH_SIZE] = (\n",
    "            out.data.clone()\n",
    "        )\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # print loss\n",
    "        i = int(i)\n",
    "        epoch = int(epoch)\n",
    "        l.append(loss.item())\n",
    "    # All batches have been trained\n",
    "    # Update the mean ensemble of outputs\n",
    "    Z = (ssl.config.alpha * Z) + (\n",
    "        1.0 - ssl.config.alpha\n",
    "    ) * outputs  # outputs + Smoothed version of Outputs\n",
    "    z = Z * (1.0 / (1.0 - ssl.config.alpha ** (epoch + 1)))\n",
    "    # print loss\n",
    "    # losses.append(np.mean(batch_losses))\n",
    "    # saving model\n",
    "    if np.mean(l) < best_loss:\n",
    "        best_loss = np.mean(l)\n",
    "        torch.save({\"state_dict\": model.state_dict()}, ssl.model_save_path)\n",
    "\n",
    "    # if (epoch + 1) % 10 == 0:\n",
    "    if True:\n",
    "        print(\n",
    "            \"Epoch [%d/%d], Training Loss: %.6f, Time (this epoch): %.2f s\"\n",
    "            % (\n",
    "                epoch + 1,\n",
    "                ssl.config.NUM_EPOCHS,\n",
    "                np.mean(l),\n",
    "                timer() - t,\n",
    "            )\n",
    "        )\n",
    "    # handle metrics, losses, etc.\n",
    "    eloss = np.mean(l)\n",
    "    losses.append(eloss)\n",
    "    sup_losses.append(\n",
    "        (1.0 / ssl.config.k) * np.sum(supl)\n",
    "    )  # division by 1/k to obtain the mean supervised loss\n",
    "    unsup_losses.append(np.mean(unsupl))\n",
    "\n",
    "    # saving model\n",
    "    if eloss < best_loss:\n",
    "        best_loss = eloss\n",
    "        torch.save({\"state_dict\": model.state_dict()}, \"model_best.pth.tar\")\n",
    "    # Log the losses in the writer\n",
    "    # Log the loss and accuracy values at the end of each epoch\n",
    "    writer.add_scalar(\"Loss/supervised_training_loss\", sup_losses[-1], epoch)\n",
    "    writer.add_scalar(\"Loss/unsupervised_training_loss\", unsup_losses[-1], epoch)\n",
    "    writer.add_scalar(\"Loss/training_loss\", losses[-1], epoch)\n",
    "\n",
    "    # test\n",
    "    # model.eval()\n",
    "    # acc = calc_metrics(model, ssl.test_loader, device=ssl.device)\n",
    "    # if self.config.print_res:\n",
    "    #     print(\n",
    "    #         \"Accuracy of the networ on the 10000 test images: %.2f %%\"\n",
    "    #         % (acc)\n",
    "    #     )\n",
    "    # # test best model\n",
    "    # checkpoint = torch.load(\"model_best.pth.tar\")\n",
    "    # model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    # model.eval()\n",
    "    # acc_best = calc_metrics(model, self.test_loader, device=self.device)\n",
    "    # if self.config.print_res:\n",
    "    #     print(\n",
    "    #         \"Accuracy of the network (best model) on the 10000 test images: %.2f %%\"\n",
    "    #         % (acc_best)\n",
    "    #     )\n",
    "# One complete forward and backward pass completes for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.671562, Time (this epoch): 593.93 s\n"
     ]
    }
   ],
   "source": [
    "# All batches have been trained\n",
    "# Update the mean ensemble of outputs\n",
    "ssl.config.alpha = 0.4\n",
    "Z = (ssl.config.alpha * Z) + (\n",
    "    1.0 - ssl.config.alpha\n",
    ") * outputs  # outputs + Smoothed version of Outputs\n",
    "z = Z * (1.0 / (1.0 - ssl.config.alpha ** (epoch + 1)))\n",
    "# print loss\n",
    "# losses.append(np.mean(batch_losses))\n",
    "# saving model\n",
    "if np.mean(l) < best_loss:\n",
    "    best_loss = np.mean(l)\n",
    "    torch.save({\"state_dict\": model.state_dict()}, ssl.model_save_path)\n",
    "\n",
    "# if (epoch + 1) % 10 == 0:\n",
    "if True:\n",
    "    print(\n",
    "        \"Epoch [%d/%d], Training Loss: %.6f, Time (this epoch): %.2f s\"\n",
    "        % (\n",
    "            epoch + 1,\n",
    "            ssl.config.NUM_EPOCHS,\n",
    "            np.mean(l),\n",
    "            timer() - t,\n",
    "        )\n",
    "    )\n",
    "# handle metrics, losses, etc.\n",
    "eloss = np.mean(l)\n",
    "losses.append(eloss)\n",
    "sup_losses.append(\n",
    "    (1.0 / ssl.config.k) * np.sum(supl)\n",
    ")  # division by 1/k to obtain the mean supervised loss\n",
    "unsup_losses.append(np.mean(unsupl))\n",
    "\n",
    "# saving model\n",
    "if eloss < best_loss:\n",
    "    best_loss = eloss\n",
    "    torch.save({\"state_dict\": model.state_dict()}, \"model_best.pth.tar\")\n",
    "# Log the losses in the writer\n",
    "# Log the loss and accuracy values at the end of each epoch\n",
    "writer.add_scalar(\"Loss/supervised_training_loss\", sup_losses[-1], epoch)\n",
    "writer.add_scalar(\"Loss/unsupervised_training_loss\", unsup_losses[-1], epoch)\n",
    "writer.add_scalar(\"Loss/training_loss\", losses[-1], epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6715622617957298"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model for each seeds\n",
    "if mode == \"train\":\n",
    "    model, _ = ssl.fit(train_loader=train_loader, test_dataset=test_dataset)\n",
    "# Test the model\n",
    "# ssl.device = torch.device(\"cpu\")\n",
    "try:\n",
    "    checkpoint = torch.load(\n",
    "        ssl.model_save_path,\n",
    "        map_location=ssl.device,\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found\")\n",
    "    # return pd.DataFrame()\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "df = evaluate_test_data(\n",
    "    model=model,\n",
    "    torch_dataset=test_dataset,\n",
    "    torch_device=ssl.device,\n",
    "    RESULT_DIR=os.path.join(RESULTS_DIR, config.experiment_name),\n",
    "    THRESHOLD=THRESHOLD,\n",
    "    save_csv_file=True,\n",
    "    save_plots=False,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
